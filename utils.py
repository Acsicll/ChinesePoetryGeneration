import json
import random
from math import log

import torch.nn.functional as F
import torch

from my_config import *

def check_vocab():
    """
    Check if the given vocabulary is valid.
    """
    with open(VOCAB_PATH, 'r', encoding='utf-8') as f:
        word2idx = json.load(f)
    idx2word = {v: k for k, v in word2idx.items()}

    print("word2idx sample:",list(word2idx.items()))
    print("idx2word sample:",list(idx2word.items()))

def nucleus_sampling(probs, p=0.9):
    """
    å®ç° Top-P (Nucleus Sampling) é‡‡æ ·ï¼š
    - é€‰æ‹©ç´¯ç§¯æ¦‚ç‡è¾¾åˆ° `p` çš„ top-n è¯æ±‡
    - ç„¶ååœ¨å…¶ä¸­éšæœºé‡‡æ ·
    """
    sorted_probs, sorted_indices = torch.sort(probs, descending=True)
    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)

    # æ‰¾åˆ°ç¬¬ä¸€ä¸ªç´¯ç§¯æ¦‚ç‡ > p çš„ä½ç½®
    cutoff_idx = (cumulative_probs > p).nonzero(as_tuple=True)[0][0]

    # åªä¿ç•™å‰ `cutoff_idx` ä¸ªæœ€é«˜æ¦‚ç‡çš„è¯
    top_p_probs = sorted_probs[:cutoff_idx + 1]
    top_p_indices = sorted_indices[:cutoff_idx + 1]

    # å½’ä¸€åŒ–åé‡‡æ ·
    top_p_probs /= top_p_probs.sum()
    sampled_index = torch.multinomial(top_p_probs, 1).item()
    return top_p_indices[sampled_index].item()

def beam_search(logits, beam_size=3):
    """Beam Search é€‰å–æœ€ä½³å€™é€‰è¯ """
    top_probs, top_indices = torch.topk(F.softmax(logits, dim=-1), k=beam_size)
    return top_indices[random.randint(0, beam_size - 1)].item()  # éšæœºä»å‰3ä¸ªé€‰ä¸€ä¸ª

def beam_search_v2(logits, beam_size=3, max_len=10, eos_token_id=2):
    candidates = [([], 0.0)]  # æ¯ä¸ªå€™é€‰åºåˆ—æ˜¯ä¸€ä¸ªå…ƒç»„ï¼š(åºåˆ—, å¯¹æ•°æ¦‚ç‡)

    for step in range(max_len):
        new_candidates = []

        for seq, log_prob in candidates:
            if len(seq) > 0 and seq[-1] == eos_token_id:
                # å¦‚æœåºåˆ—å·²ç»ç»“æŸï¼Œç›´æ¥ä¿ç•™
                new_candidates.append((seq, log_prob))
                continue

            # è·å–å½“å‰æ—¶é—´æ­¥çš„ logits
            #print("logits:",logits.shape)
            current_logits = logits.squeeze(0) # [1, vocab_size]
            probs = F.softmax(current_logits, dim=-1)  # [1, vocab_size]

            # é€‰æ‹© top-k å€™é€‰è¯
            top_probs, top_indices = torch.topk(probs, k=beam_size)  # [batch_size, beam_size]

            for i in range(beam_size):
                next_token = top_indices[i].item()  # é€‰æ‹©ç¬¬ i ä¸ªå€™é€‰è¯
                next_log_prob = log(top_probs[i].item())  # è®¡ç®—å¯¹æ•°æ¦‚ç‡

                new_seq = seq + [next_token]
                new_log_prob = log_prob + next_log_prob

                new_candidates.append((new_seq, new_log_prob))

        # æŒ‰å¯¹æ•°æ¦‚ç‡æ’åºï¼Œé€‰æ‹© top-k å€™é€‰åºåˆ—
        new_candidates.sort(key=lambda x: x[1], reverse=True)
        candidates = new_candidates[:beam_size]

    # è¿”å›æœ€ä¼˜çš„å€™é€‰åºåˆ—
    best_sequence = candidates[0][0]
    return best_sequence

def beam_search_v3(logits, beam_size=4, max_len=10, eos_token_id=2, length_penalty=0.6):

    candidates = [([], 0.0)]  # æ¯ä¸ªå€™é€‰åºåˆ—æ˜¯ä¸€ä¸ªå…ƒç»„ï¼š(åºåˆ—, å¯¹æ•°æ¦‚ç‡)

    for step in range(max_len):
        new_candidates = []

        for seq, log_prob in candidates:
            if len(seq) > 0 and seq[-1] == eos_token_id:
                # å¦‚æœåºåˆ—å·²ç»ç»“æŸï¼Œç›´æ¥ä¿ç•™
                new_candidates.append((seq, log_prob))
                continue

            # è·å–å½“å‰æ—¶é—´æ­¥çš„ logits
            current_logits = logits.squeeze(0)  # [1, vocab_size]
            probs = F.softmax(current_logits, dim=-1)  # [1, vocab_size]

            # é€‰æ‹© top-k å€™é€‰è¯
            top_probs, top_indices = torch.topk(probs, k=beam_size)  # [batch_size, beam_size]

            for i in range(beam_size):
                next_token = top_indices[i].item()  # é€‰æ‹©ç¬¬ i ä¸ªå€™é€‰è¯
                next_log_prob = log(top_probs[i].item())  # è®¡ç®—å¯¹æ•°æ¦‚ç‡

                # é•¿åº¦å½’ä¸€åŒ–
                length_penalized_log_prob = log_prob + next_log_prob / (len(seq) + 1) ** length_penalty

                new_seq = seq + [next_token]
                new_candidates.append((new_seq, length_penalized_log_prob))

        # æŒ‰å¯¹æ•°æ¦‚ç‡æ’åºï¼Œé€‰æ‹© top-k å€™é€‰åºåˆ—
        new_candidates.sort(key=lambda x: x[1], reverse=True)
        candidates = new_candidates[:beam_size]

    # è¿”å›æœ€ä¼˜çš„å€™é€‰åºåˆ—
    best_sequence = candidates[0][0]
    return best_sequence



def fix_poem_rhythm(poem):
    """
    ğŸµ åå¤„ç†ï¼šæ£€æŸ¥éŸµå¾‹ï¼ˆå¯é€‰ï¼‰
    - ç¡®ä¿å¶æ•°è¡Œå¯¹ä»—
    - è°ƒæ•´å¥å°¾å­—éŸµè„š
    """
    # TODO: è¿™é‡Œå¯ä»¥åŸºäºéŸ³éŸµåº“è¿›è¡Œä¼˜åŒ–ï¼Œç›®å‰åªæ˜¯ç®€å•è¿”å›
    return poem


if __name__=="__main__":
    check_vocab()
